# Asyncyhronous Methods for Deep Reinforcement Learning


## Introduction


### RL and Deep RL refresher

Reinforcement learning algorithms evaluate the value of a state and a set of actions given that state, performing the action that is predicted to give the greatest expected reward. Deep learning is introduced to evaluate the value of a state and the expected reward given the actions avaliable

== It was prev. thought that deep rl was unstable due to following reasons ==

- the sequence of data encountered by an online rl agent (as it explores the space) is non-stationary (since it is online, it is changing it's behaviour with every update) 
- online rl updates are strongly correlated (an update to the current network is very dependent on the previous update)

 == Solutions to this followed the common idea (experience replay memory) ==

- store the agent's data in an "experience replay memory", and we can learn by batching or randomly sampling the data from random timesteps
- Reduces the non-stationarity and decorrelates updates, but limits methods to off-policy RL algos

== Drawbacks ==
- used more mem and computation per real interraction, req's off-policy learning algos which can update from data generated by an older policy


### Async DRL


Instead of experience replay, execute multiple agents in parallel on their own instances of the env

== Benefits ==
- Decorrelates the agent's data into a more stationary process, as each agent is experiencing a different environment
	- Allows for more on-policy RL algos
- Can train on a standard multi-core CPU


## RL Background

Consider a standard RL environment where an agent interacts with an environment \Epsilon over a number of discrete timesteps.

for each time step t, the agent gets state s_t and selects action a_t (a_t from set of actions) according to policy \pi, which is a function which takes state s_t and return action a_t. Agent gets s_(t+1) and reward r_t. Eventually an end state is reached, and the return R_t is \sum_{k=0}^\infty \gamma^k * r_{t+k}. Goal of agent is to maximize R_t.

 == Deep learning comes in to be the action-value function. This is either ==
- Q^\pi(s,a) = E[R_t | s_t, a] (expected return for selecting action a in state s and then following policy pi)
- V^\pi(s)   = E[R_t | s_t = s] (expected return for following policy \pi from state s)

 == Value- vs. Policy-based methods ==
- Value-based methods approximate Q(s,a;\theta), where \theta is the weights of the network
	- Includes n-step returns to propogate rewards faster
- Policy-based methods approximate \pi(a|s;\theta)
	- typically updates \pi by following gradient ascent on E[R_t]
	- e.g. REINFORCE algorithms update in direction of \grad_\theta log(\pi(a_t | s_t;\theta) R_t, which is an unbiased estimator for E[R_t]
	- reduce variance by subtracting a learned function of the state b_t(s_t) from the return; so resulting gradient is \grad_\theta log(\pi(a_t | s_t;\theta) (R_t - b_t(s_t))

 == Actor Critic ==
b_t(s_t) is approximately V^\pi(s_t). When an approximate value function is used as the baseline, (R_t - b_t(s_t)) can be seen as an estimate of the advantage of a_t in s_t, as R_t is an estimate of Q^\pi and b_t is an estimate of V^\pi. This is the Actor-Critic, where The policy \pi is the actor and the baseline \b_t is the critic.


